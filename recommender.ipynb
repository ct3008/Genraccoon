{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Claudia/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m34/34\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step\n",
      "\n",
      "üéµ Recommendation for: Cien Anos\n",
      "                track_id                            title   artist_name  \\\n",
      "5502  TRAXXGI128F14688C7  Paper Tigers (acoustic version)  Tom Cochrane   \n",
      "1200  TRAGROY128F92D2C53    This Kind Of Love (Full Band)  Sister Hazel   \n",
      "9969  TRBBLFA12903CDC496                    I'll Be Yours   Faron Young   \n",
      "5608  TRAEUVH128F422559A                   Hasta El Final  La Portuaria   \n",
      "1850  TRATNHC12903CFA949                          Hey Joe    Carl Smith   \n",
      "\n",
      "     predicted_genre                                top_5_similar_songs  \n",
      "5502       Blues Rap  TRAXWQU128F42716E2; TRBBHPQ128F145A7F8; TRAHKR...  \n",
      "1200       Blues Rap  TRBHNCW128F9334F3D; TRAAJJG128F4284B27; TRAYLR...  \n",
      "9969       Blues Rap  TRAOXWF128E0790985; TRADCBA128F92E7161; TRBHFD...  \n",
      "5608       Blues Rap  TRAGKAC128F4225537; TRACYOR128F427FB1D; TRAGMX...  \n",
      "1850       Blues Rap  TRAUREN128F931F5AB; TRBBLFA12903CDC496; TRATES...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, GRU, Dense, Dropout, Bidirectional, Concatenate, Layer\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "with open(\"tokenizer.pkl\", \"rb\") as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "\n",
    "\n",
    "class AttentionLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.Wa = self.add_weight(name='Wa', shape=(input_shape[-1], input_shape[-1]), initializer='glorot_uniform', trainable=True)\n",
    "        self.ba = self.add_weight(name='ba', shape=(input_shape[-1],), initializer='zeros', trainable=True)\n",
    "        self.va = self.add_weight(name='va', shape=(input_shape[-1], 1), initializer='glorot_uniform', trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        uit = tf.tanh(tf.tensordot(inputs, self.Wa, axes=1) + self.ba)\n",
    "        ait = tf.nn.softmax(tf.tensordot(uit, self.va, axes=1), axis=1)\n",
    "        weighted_input = inputs * ait\n",
    "        return tf.reduce_sum(weighted_input, axis=1)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(AttentionLayer, self).get_config()\n",
    "        return config\n",
    "\n",
    "\n",
    "# --- Load & preprocess full dataset ---\n",
    "nltk.download('stopwords')\n",
    "df = pd.read_csv(\"msd_dataset_enriched_with_similar_songs.csv\")\n",
    "df = df[~df['lyrics'].isna() & ~df['lyrics'].str.strip().eq(\"\")]\n",
    "stopwords_set = set(stopwords.words('english'))\n",
    "\n",
    "# Clean lyrics\n",
    "def clean_lyrics(text):\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text.lower())\n",
    "    tokens = text.split()\n",
    "    return ' '.join([t for t in tokens if t not in stopwords_set])\n",
    "\n",
    "df['cleaned_lyrics'] = df['lyrics'].apply(clean_lyrics)\n",
    "\n",
    "# Define meta features\n",
    "meta_features = ['duration','tempo','key','loudness'] + [\n",
    "    f'pitch_mean_{i}' for i in range(12)\n",
    "] + [f'pitch_std_{i}' for i in range(12)] + [f'timbre_mean_{i}' for i in range(12)] + [f'timbre_std_{i}' for i in range(12)]\n",
    "\n",
    "# Fill NA and standardize meta features\n",
    "scaler = StandardScaler()\n",
    "df[meta_features] = df[meta_features].fillna(0)\n",
    "X_meta_all = scaler.fit_transform(df[meta_features])\n",
    "\n",
    "\n",
    "# Tokenize lyrics\n",
    "# tokenizer = Tokenizer()\n",
    "# tokenizer.fit_on_texts(df['cleaned_lyrics'])\n",
    "X_lyrics_all = pad_sequences(tokenizer.texts_to_sequences(df['cleaned_lyrics']), maxlen=100)\n",
    "\n",
    "# Load trained genre classifier model\n",
    "best_model = load_model(\"gru_atten_model.h5\", custom_objects={'AttentionLayer': AttentionLayer})\n",
    "  # assumes model is saved here\n",
    "label_encoder = LabelEncoder()\n",
    "# label_encoder.fit(df[df['genre'].notna()]['genre'])\n",
    "genre_labels = df[df['genre'].notna()]['genre']\n",
    "if len(genre_labels) == 0:\n",
    "    raise ValueError(\"No known genre labels available to train label encoder.\")\n",
    "label_encoder.fit(genre_labels)\n",
    "\n",
    "\n",
    "# Predict genres for unlabeled data\n",
    "df['predicted_genre'] = df['genre']\n",
    "unlabeled_mask = df['genre'].isna()\n",
    "y_pred = np.argmax(best_model.predict([X_lyrics_all[unlabeled_mask], X_meta_all[unlabeled_mask]]), axis=1)\n",
    "df.loc[unlabeled_mask, 'predicted_genre'] = label_encoder.inverse_transform(y_pred)\n",
    "\n",
    "# Build cosine similarity matrix\n",
    "genre_vectors = X_meta_all\n",
    "cos_sim_matrix = cosine_similarity(genre_vectors)\n",
    "\n",
    "# Recommend top 5 songs based on genre and similarity\n",
    "def recommend_similar_songs(song_index, top_k=5):\n",
    "    \n",
    "    target_genre = df.iloc[song_index]['predicted_genre']\n",
    "    target_similarities = cos_sim_matrix[song_index]\n",
    "\n",
    "    same_genre_mask = df['predicted_genre'] == target_genre\n",
    "    target_similarities[~same_genre_mask] = -1  # mask different genres\n",
    "\n",
    "    top_indices = np.argsort(target_similarities)[::-1][1:top_k+1]  # skip self\n",
    "    return df.iloc[top_indices][['track_id', 'title', 'artist_name', 'predicted_genre', 'top_5_similar_songs']]\n",
    "\n",
    "# Example\n",
    "index = 123  # sample index to test\n",
    "print(\"\\nüéµ Recommendation for:\", df.iloc[index]['title'])\n",
    "print(recommend_similar_songs(index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test songs that appear as similar songs: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"msd_dataset_enriched_with_similar_songs.csv\")\n",
    "\n",
    "# Clean any missing values\n",
    "df = df[~df['track_id'].isna()]\n",
    "df = df[~df['top_5_similar_songs'].isna()]\n",
    "\n",
    "# Normalize and split similar song lists\n",
    "def extract_similar_ids(sim_str):\n",
    "    return [s.strip() for s in sim_str.split(\";\") if s.strip()]\n",
    "\n",
    "# Build a set of all referenced song IDs\n",
    "referenced_ids = set()\n",
    "df['similar_ids'] = df['top_5_similar_songs'].apply(extract_similar_ids)\n",
    "for ids in df['similar_ids']:\n",
    "    referenced_ids.update(ids)\n",
    "\n",
    "# Identify songs that are *not* referenced as similar\n",
    "df['is_referenced'] = df['track_id'].apply(lambda tid: tid in referenced_ids)\n",
    "\n",
    "# Try to put less referenced songs into the test set\n",
    "not_referenced_df = df[~df['is_referenced']]\n",
    "referenced_df = df[df['is_referenced']]\n",
    "\n",
    "# Calculate 30% of total rows for test set\n",
    "test_size = int(0.3 * len(df))\n",
    "\n",
    "# Start with as many non-referenced songs as possible in the test set\n",
    "n_test_from_not_referenced = min(test_size, len(not_referenced_df))\n",
    "test_df = not_referenced_df.sample(n=n_test_from_not_referenced, random_state=42)\n",
    "\n",
    "# Fill the remaining test slots from the referenced ones\n",
    "remaining_test_size = test_size - len(test_df)\n",
    "if remaining_test_size > 0:\n",
    "    test_df = pd.concat([\n",
    "        test_df,\n",
    "        referenced_df.sample(n=remaining_test_size, random_state=42)\n",
    "    ])\n",
    "\n",
    "# Remaining data is training set\n",
    "train_df = df.drop(test_df.index)\n",
    "\n",
    "# Optional: check overlap\n",
    "overlap = set(test_df['track_id']) & referenced_ids\n",
    "print(f\"Number of test songs that appear as similar songs: {len(overlap)}\")\n",
    "\n",
    "# Save to CSV\n",
    "train_df.to_csv(\"train_set.csv\", index=False)\n",
    "test_df.to_csv(\"test_set.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Claudia/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m24/24\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step\n",
      "Generating song embeddings...\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 58ms/step\n",
      "Embeddings generated: (1716, 320)\n",
      "Building positive and negative pairs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1716/1716 [03:23<00:00,  8.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done building pairs.\n",
      "Number of training pairs created: 3042\n",
      "Starting training of similarity model...\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5615 - loss: 0.6879 - val_accuracy: 0.9261 - val_loss: 0.4707\n",
      "Epoch 2/10\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6159 - loss: 0.6487 - val_accuracy: 0.6634 - val_loss: 0.6128\n",
      "Epoch 3/10\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6458 - loss: 0.6259 - val_accuracy: 0.6716 - val_loss: 0.6124\n",
      "Training complete.\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\n",
      "üéµ Test Recommendation for: C'est pas ma faute\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "                track_id                            title          artist_name\n",
      "5817  TRBAKRM128F4248228        The End. [Live In Mexico]  My Chemical Romance\n",
      "2211  TRAFLZY128F148959F  House Of Wolves (Album Version)  My Chemical Romance\n",
      "1574  TRASUJK128E0789C12            Speed Of Sound (Live)             Coldplay\n",
      "4618  TRAKLHO128F42BA554                        Rocks Off   The Rolling Stones\n",
      "3988  TRAXJHG128F427EA02                     Be Like That         3 Doors Down\n"
     ]
    }
   ],
   "source": [
    "# Supervised Song Similarity Recommender\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Concatenate, GRU, Embedding, Bidirectional, Layer\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import tensorflow as tf\n",
    "import ast\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "nltk.download('stopwords')\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Attention Layer\n",
    "class AttentionLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.Wa = self.add_weight(name='Wa', shape=(input_shape[-1], input_shape[-1]), initializer='glorot_uniform', trainable=True)\n",
    "        self.ba = self.add_weight(name='ba', shape=(input_shape[-1],), initializer='zeros', trainable=True)\n",
    "        self.va = self.add_weight(name='va', shape=(input_shape[-1], 1), initializer='glorot_uniform', trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        uit = tf.tanh(tf.tensordot(inputs, self.Wa, axes=1) + self.ba)\n",
    "        ait = tf.nn.softmax(tf.tensordot(uit, self.va, axes=1), axis=1)\n",
    "        weighted_input = inputs * ait\n",
    "        return tf.reduce_sum(weighted_input, axis=1)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(AttentionLayer, self).get_config()\n",
    "        return config\n",
    "\n",
    "# --- Load Data ---\n",
    "# df = pd.read_csv(\"msd_dataset_enriched_with_similar_songs.csv\")\n",
    "df = pd.read_csv(\"train_set.csv\")\n",
    "df = df[~df['lyrics'].isna() & df['lyrics'].str.strip().ne(\"\")]\n",
    "df['top_5_similar_songs'] = df['top_5_similar_songs'].apply(\n",
    "    lambda x: [s.strip() for s in x.split(';')] if isinstance(x, str) and ';' in x else []\n",
    ")\n",
    "\n",
    "# --- Preprocessing ---\n",
    "stopwords_set = set(stopwords.words('english'))\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", str(text).lower())\n",
    "    return ' '.join(word for word in text.split() if word not in stopwords_set)\n",
    "\n",
    "df['cleaned_lyrics'] = df['lyrics'].apply(clean_text)\n",
    "df['cleaned_title'] = df['title'].apply(clean_text)\n",
    "df['cleaned_tags'] = df['artist_tags'].apply(clean_text)\n",
    "\n",
    "# --- Predict missing genres using pretrained model ---\n",
    "with open(\"tokenizer.pkl\", \"rb\") as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "\n",
    "X_lyrics_all = pad_sequences(tokenizer.texts_to_sequences(df['cleaned_lyrics']), maxlen=100)\n",
    "\n",
    "# Only meta features used by pretrained genre model\n",
    "genre_meta_features = ['duration','tempo','key','loudness'] + [\n",
    "    f'pitch_mean_{i}' for i in range(12)\n",
    "] + [f'pitch_std_{i}' for i in range(12)] + [f'timbre_mean_{i}' for i in range(12)] + [f'timbre_std_{i}' for i in range(12)]\n",
    "df[genre_meta_features] = df[genre_meta_features].fillna(0)\n",
    "meta_scaler = StandardScaler()\n",
    "X_meta_all = meta_scaler.fit_transform(df[genre_meta_features])\n",
    "\n",
    "best_model = load_model(\"gru_atten_model.h5\", custom_objects={'AttentionLayer': AttentionLayer})\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(df[df['genre'].notna()]['genre'])\n",
    "\n",
    "unlabeled_mask = df['genre'].isna()\n",
    "y_pred = np.argmax(best_model.predict([X_lyrics_all[unlabeled_mask], X_meta_all[unlabeled_mask]]), axis=1)\n",
    "classes = label_encoder.classes_\n",
    "safe_y_pred = [classes[i] if i < len(classes) else \"Unknown\" for i in y_pred]\n",
    "df.loc[unlabeled_mask, 'predicted_genre'] = safe_y_pred\n",
    "\n",
    "# --- Use all relevant features for similarity model ---\n",
    "exclude_cols = {'track_id', 'artist_name', 'artist_id', 'top_5_similar_songs', 'genre', 'lyrics', 'song_id', 'release', 'title', 'artist_tags'}\n",
    "categorical_cols = ['predicted_genre']\n",
    "\n",
    "all_feature_cols = [col for col in df.columns if col not in exclude_cols and col not in categorical_cols]\n",
    "\n",
    "# One-hot encode predicted genre\n",
    "df['predicted_genre'] = df['predicted_genre'].astype(str)\n",
    "ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "genre_encoded = ohe.fit_transform(df[['predicted_genre']])\n",
    "\n",
    "# Scale only numeric features\n",
    "feature_data = df[all_feature_cols].select_dtypes(include=[np.number]).fillna(0)\n",
    "scaler = StandardScaler()\n",
    "feature_scaled = scaler.fit_transform(feature_data)\n",
    "X_meta = np.hstack([feature_scaled, genre_encoded])\n",
    "\n",
    "# Lyrics input\n",
    "X_lyrics = pad_sequences(tokenizer.texts_to_sequences(df['cleaned_lyrics']), maxlen=100)\n",
    "X_title = pad_sequences(tokenizer.texts_to_sequences(df['cleaned_title']), maxlen=20)\n",
    "X_tags = pad_sequences(tokenizer.texts_to_sequences(df['cleaned_tags']), maxlen=30)\n",
    "\n",
    "# --- Build song embeddings (lyrics + meta + title + tags) ---\n",
    "embedding_dim = 50\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "lyrics_input = Input(shape=(100,), name='lyrics_input')\n",
    "lyrics_embed = Embedding(input_dim=vocab_size, output_dim=embedding_dim, trainable=True)(lyrics_input)\n",
    "lyrics_output = Bidirectional(GRU(64))(lyrics_embed)\n",
    "\n",
    "meta_input = Input(shape=(X_meta.shape[1],), name='meta_input')\n",
    "meta_output = Dense(64, activation='relu')(meta_input)\n",
    "\n",
    "title_input = Input(shape=(20,), name='title_input')\n",
    "title_embed = Embedding(input_dim=vocab_size, output_dim=embedding_dim, trainable=True)(title_input)\n",
    "title_output = Bidirectional(GRU(32))(title_embed)\n",
    "\n",
    "tags_input = Input(shape=(30,), name='tags_input')\n",
    "tags_embed = Embedding(input_dim=vocab_size, output_dim=embedding_dim, trainable=True)(tags_input)\n",
    "tags_output = Bidirectional(GRU(32))(tags_embed)\n",
    "\n",
    "combined = Concatenate()([lyrics_output, meta_output, title_output, tags_output])\n",
    "embedding_model = Model(inputs=[lyrics_input, meta_input, title_input, tags_input], outputs=combined)\n",
    "\n",
    "print(\"Generating song embeddings...\")\n",
    "song_embeddings = embedding_model.predict([X_lyrics, X_meta, X_title, X_tags], batch_size=64, verbose=1)\n",
    "print(\"Embeddings generated:\", song_embeddings.shape)\n",
    "\n",
    "# --- Build pairs ---\n",
    "def build_pairs(df, top_k=5, neg_ratio=2):\n",
    "    print(\"Building positive and negative pairs...\")\n",
    "    pos_pairs, neg_pairs = [], []\n",
    "    track_to_index = {tid: i for i, tid in enumerate(df['track_id'])}\n",
    "    for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        anchor = i\n",
    "        pos_added = 0\n",
    "        for sim_id in row['top_5_similar_songs']:\n",
    "            if sim_id in track_to_index:\n",
    "                sim_idx = track_to_index[sim_id]\n",
    "                if sim_idx < len(df):\n",
    "                    pos_pairs.append((anchor, sim_idx, 1))\n",
    "                    pos_added += 1\n",
    "\n",
    "        if pos_added == 0:\n",
    "            continue\n",
    "\n",
    "        all_indices = list(range(len(df)))\n",
    "        valid_negatives = [j for j in all_indices if j != anchor and df.iloc[j]['track_id'] not in row['top_5_similar_songs']]\n",
    "        num_neg = min(neg_ratio * pos_added, len(valid_negatives))\n",
    "        negatives = random.sample(valid_negatives, k=num_neg)\n",
    "        for neg in negatives:\n",
    "            if neg < len(df):\n",
    "                neg_pairs.append((anchor, neg, 0))\n",
    "    print(\"Done building pairs.\")\n",
    "    return pos_pairs + neg_pairs\n",
    "\n",
    "pairs = build_pairs(df)\n",
    "\n",
    "# --- Create training data ---\n",
    "X_pair = []\n",
    "y_pair = []\n",
    "\n",
    "for anchor_idx, compare_idx, label in pairs:\n",
    "    if anchor_idx >= len(song_embeddings) or compare_idx >= len(song_embeddings):\n",
    "        continue  # skip out-of-bounds indices\n",
    "    anchor_vec = song_embeddings[anchor_idx]\n",
    "    compare_vec = song_embeddings[compare_idx]\n",
    "\n",
    "    features = np.concatenate([anchor_vec, compare_vec, np.abs(anchor_vec - compare_vec)])\n",
    "    X_pair.append(features)\n",
    "    y_pair.append(label)\n",
    "\n",
    "X_pair = np.array(X_pair)\n",
    "y_pair = np.array(y_pair)\n",
    "print(\"Number of training pairs created:\", len(X_pair))\n",
    "\n",
    "# --- Similarity Classifier ---\n",
    "sim_input = Input(shape=(X_pair.shape[1],))\n",
    "z = Dense(128, activation='relu')(sim_input)\n",
    "z = Dropout(0.3)(z)\n",
    "z = Dense(64, activation='relu')(z)\n",
    "out = Dense(1, activation='sigmoid')(z)\n",
    "\n",
    "sim_model = Model(sim_input, out)\n",
    "sim_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"Starting training of similarity model...\")\n",
    "sim_model.fit(X_pair, y_pair, batch_size=64, epochs=10, validation_split=0.2, verbose=1,\n",
    "              callbacks=[EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)])\n",
    "print(\"Training complete.\")\n",
    "\n",
    "\n",
    "\n",
    "# --- Save models and preprocessing objects ---\n",
    "embedding_model.save(\"embedding_model.keras\")\n",
    "sim_model.save(\"similarity_model.keras\")\n",
    "\n",
    "with open(\"tokenizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "with open(\"scaler.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scaler, f)\n",
    "with open(\"meta_scaler.pkl\", \"wb\") as f:\n",
    "    pickle.dump(meta_scaler, f)\n",
    "with open(\"ohe.pkl\", \"wb\") as f:\n",
    "    pickle.dump(ohe, f)\n",
    "with open(\"label_encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "np.save(\"song_embeddings.npy\", song_embeddings)\n",
    "\n",
    "\n",
    "def preprocess_test_df(df_test):\n",
    "    print(\"Preprocessing test set...\")\n",
    "    df_test = df_test[~df_test['lyrics'].isna() & df_test['lyrics'].str.strip().ne(\"\")].copy()\n",
    "\n",
    "    # Clean and transform\n",
    "    for col in ['lyrics', 'title', 'artist_tags']:\n",
    "        df_test[f'cleaned_{col}'] = df_test[col].apply(clean_text)\n",
    "\n",
    "    X_lyrics = pad_sequences(tokenizer.texts_to_sequences(df_test['cleaned_lyrics']), maxlen=100)\n",
    "    X_title = pad_sequences(tokenizer.texts_to_sequences(df_test['cleaned_title']), maxlen=20)\n",
    "    X_tags = pad_sequences(tokenizer.texts_to_sequences(df_test['cleaned_tags']), maxlen=30)\n",
    "\n",
    "    feature_data = df_test[all_feature_cols].select_dtypes(include=[np.number]).fillna(0)\n",
    "    feature_scaled = scaler.transform(feature_data)\n",
    "\n",
    "    # Fill missing genres\n",
    "    df_test['predicted_genre'] = df_test['genre']\n",
    "    unlabeled_mask = df_test['genre'].isna()\n",
    "    if unlabeled_mask.sum() > 0:\n",
    "        X_meta_test_for_genre = meta_scaler.transform(df_test[genre_meta_features].fillna(0))\n",
    "        X_lyrics_for_genre = pad_sequences(tokenizer.texts_to_sequences(df_test['cleaned_lyrics']), maxlen=100)\n",
    "        y_pred_test = np.argmax(best_model.predict([X_lyrics_for_genre[unlabeled_mask], X_meta_test_for_genre[unlabeled_mask]]), axis=1)\n",
    "        df_test.loc[unlabeled_mask, 'predicted_genre'] = [label_encoder.classes_[i] if i < len(label_encoder.classes_) else \"Unknown\" for i in y_pred_test]\n",
    "\n",
    "    genre_ohe = ohe.transform(df_test[['predicted_genre']].astype(str))\n",
    "    X_meta = np.hstack([feature_scaled, genre_ohe])\n",
    "\n",
    "    return df_test, [X_lyrics, X_meta, X_title, X_tags]\n",
    "\n",
    "def recommend_similar_from_training(song_index, test_embeddings, top_k=5):\n",
    "    target_vec = test_embeddings[song_index]\n",
    "    scores = []\n",
    "    for i in tqdm(range(len(song_embeddings)), desc=\"Scoring similarities\"):\n",
    "        comp_vec = song_embeddings[i]\n",
    "        pair_vec = np.concatenate([target_vec, comp_vec, np.abs(target_vec - comp_vec)])\n",
    "        score = sim_model.predict(np.expand_dims(pair_vec, axis=0), verbose=0)[0][0]\n",
    "        scores.append((i, score))\n",
    "    top_matches = sorted(scores, key=lambda x: x[1], reverse=True)[:top_k]\n",
    "    return df.iloc[[i for i, _ in top_matches]][['track_id', 'title', 'artist_name']]\n",
    "\n",
    "\n",
    "df_test = pd.read_csv(\"test_set.csv\")\n",
    "df_test, test_inputs = preprocess_test_df(df_test)\n",
    "\n",
    "print(\"\\nüéµ Test Recommendation for:\", df_test.iloc[0]['title'])\n",
    "print(recommend_similar_from_training(0, embedding_model.predict(test_inputs)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Claudia/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m34/34\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "Preprocessing test set...\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 75ms/step\n",
      "Evaluating and generating recommendations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 622/622 [8:46:46<00:00, 50.81s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete. Results saved to test_recommendations.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Supervised Song Similarity Recommender\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Concatenate, GRU, Embedding, Bidirectional, Layer\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import tensorflow as tf\n",
    "import ast\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "nltk.download('stopwords')\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# --- Load saved components ---\n",
    "embedding_model = load_model(\"embedding_model.keras\")\n",
    "sim_model = load_model(\"similarity_model.keras\")\n",
    "song_embeddings = np.load(\"song_embeddings.npy\")\n",
    "\n",
    "with open(\"tokenizer.pkl\", \"rb\") as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "with open(\"scaler.pkl\", \"rb\") as f:\n",
    "    scaler = pickle.load(f)\n",
    "with open(\"meta_scaler.pkl\", \"rb\") as f:\n",
    "    meta_scaler = pickle.load(f)\n",
    "with open(\"ohe.pkl\", \"rb\") as f:\n",
    "    ohe = pickle.load(f)\n",
    "with open(\"label_encoder.pkl\", \"rb\") as f:\n",
    "    label_encoder = pickle.load(f)\n",
    "# with open(\"gru_atten_model.h5\", \"rb\") as f:\n",
    "#     best_model = load_model(f, custom_objects={'AttentionLayer': Layer})\n",
    "best_model = load_model(\"gru_atten_model.h5\", custom_objects={'AttentionLayer': AttentionLayer})\n",
    "\n",
    "\n",
    "# --- Reuse genre features list and cleaner ---\n",
    "genre_meta_features = ['duration','tempo','key','loudness'] + [\n",
    "    f'pitch_mean_{i}' for i in range(12)\n",
    "] + [f'pitch_std_{i}' for i in range(12)] + [f'timbre_mean_{i}' for i in range(12)] + [f'timbre_std_{i}' for i in range(12)]\n",
    "\n",
    "stopwords_set = set(stopwords.words('english'))\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", str(text).lower())\n",
    "    return ' '.join(word for word in text.split() if word not in stopwords_set)\n",
    "\n",
    "exclude_cols = {'track_id', 'artist_name', 'artist_id', 'top_5_similar_songs', 'genre', 'lyrics', 'song_id', 'release', 'title', 'artist_tags'}\n",
    "categorical_cols = ['predicted_genre']\n",
    "\n",
    "# Load full dataset to get consistent feature columns\n",
    "df_full = pd.read_csv(\"msd_dataset_enriched_with_similar_songs.csv\")\n",
    "df_full = df_full[~df_full['lyrics'].isna() & df_full['lyrics'].str.strip().ne(\"\")].copy()\n",
    "df_full['genre'] = df_full['genre'].fillna(\"Unknown\")\n",
    "\n",
    "# Predict genres for missing entries in full set\n",
    "X_meta_all = meta_scaler.transform(df_full[genre_meta_features].fillna(0))\n",
    "X_lyrics_all = pad_sequences(tokenizer.texts_to_sequences(df_full['lyrics'].apply(clean_text)), maxlen=100)\n",
    "unlabeled_mask = df_full['genre'] == \"Unknown\"\n",
    "y_pred = np.argmax(best_model.predict([X_lyrics_all[unlabeled_mask], X_meta_all[unlabeled_mask]]), axis=1)\n",
    "df_full.loc[unlabeled_mask, 'genre'] = label_encoder.inverse_transform(y_pred)\n",
    "df_full['predicted_genre'] = df_full['genre']\n",
    "\n",
    "all_feature_cols = [col for col in df_full.columns if col not in exclude_cols and col not in categorical_cols]\n",
    "\n",
    "# --- Preprocess Test Data ---\n",
    "def preprocess_test_df(df_test):\n",
    "    print(\"Preprocessing test set...\")\n",
    "    df_test = df_test[~df_test['lyrics'].isna() & df_test['lyrics'].str.strip().ne(\"\")].copy()\n",
    "\n",
    "    for col in ['lyrics', 'title', 'artist_tags']:\n",
    "        df_test[f'cleaned_{col}'] = df_test[col].apply(clean_text)\n",
    "\n",
    "    X_lyrics = pad_sequences(tokenizer.texts_to_sequences(df_test['cleaned_lyrics']), maxlen=100)\n",
    "    X_title = pad_sequences(tokenizer.texts_to_sequences(df_test['cleaned_title']), maxlen=20)\n",
    "    X_tags = pad_sequences(tokenizer.texts_to_sequences(df_test['cleaned_artist_tags']), maxlen=30)\n",
    "\n",
    "    feature_data = df_test[all_feature_cols].select_dtypes(include=[np.number]).fillna(0)\n",
    "    feature_scaled = scaler.transform(feature_data)\n",
    "\n",
    "    # Predict genre if missing\n",
    "    df_test['predicted_genre'] = df_test['genre']\n",
    "    unlabeled_mask = df_test['genre'].isna()\n",
    "    if unlabeled_mask.sum() > 0:\n",
    "        X_meta_test_for_genre = meta_scaler.transform(df_test[genre_meta_features].fillna(0))\n",
    "        X_lyrics_for_genre = pad_sequences(tokenizer.texts_to_sequences(df_test['cleaned_lyrics']), maxlen=100)\n",
    "        y_pred_test = np.argmax(best_model.predict([X_lyrics_for_genre[unlabeled_mask], X_meta_test_for_genre[unlabeled_mask]]), axis=1)\n",
    "        df_test.loc[unlabeled_mask, 'predicted_genre'] = label_encoder.inverse_transform(y_pred_test)\n",
    "\n",
    "    genre_ohe = ohe.transform(df_test[['predicted_genre']].astype(str))\n",
    "    X_meta = np.hstack([feature_scaled, genre_ohe])\n",
    "\n",
    "    return df_test, [X_lyrics, X_meta, X_title, X_tags]\n",
    "\n",
    "# --- Predict All Recommendations with Incremental Saving ---\n",
    "def evaluate_recommendations(df_test, test_embeddings, output_file=\"test_recommendations.csv\", top_k=5):\n",
    "    print(\"Evaluating and generating recommendations...\")\n",
    "\n",
    "    already_written = set()\n",
    "    if os.path.exists(output_file):\n",
    "        existing_df = pd.read_csv(output_file)\n",
    "        already_written = set(existing_df['test_track_id'])\n",
    "        mode = 'a'\n",
    "        header = False\n",
    "    else:\n",
    "        mode = 'w'\n",
    "        header = True\n",
    "\n",
    "    with open(output_file, mode, newline='') as f:\n",
    "        for idx in tqdm(range(len(df_test))):\n",
    "            track_id = df_test.iloc[idx]['track_id']\n",
    "            if track_id in already_written:\n",
    "                continue\n",
    "\n",
    "            target_vec = test_embeddings[idx]\n",
    "            scores = []\n",
    "            for i in range(len(song_embeddings)):\n",
    "                comp_vec = song_embeddings[i]\n",
    "                pair_vec = np.concatenate([target_vec, comp_vec, np.abs(target_vec - comp_vec)])\n",
    "                score = sim_model.predict(np.expand_dims(pair_vec, axis=0), verbose=0)[0][0]\n",
    "                scores.append((i, score))\n",
    "\n",
    "            top_matches = sorted(scores, key=lambda x: x[1], reverse=True)[:top_k]\n",
    "            top_songs = df_full.iloc[[i for i, _ in top_matches]][['track_id', 'title', 'artist_name', 'genre']]\n",
    "            match_ids = top_songs['track_id'].astype(str).tolist()\n",
    "            match_titles = top_songs['title'].astype(str).tolist()\n",
    "            match_artists = top_songs['artist_name'].astype(str).tolist()\n",
    "            match_genres = top_songs['genre'].astype(str).tolist()\n",
    "\n",
    "            true_matches = df_test.iloc[idx]['top_5_similar_songs'].split(';') if isinstance(df_test.iloc[idx]['top_5_similar_songs'], str) else []\n",
    "            hit = any(pred in true_matches for pred in match_ids)\n",
    "\n",
    "            line = pd.DataFrame([{\n",
    "                'test_track_id': track_id,\n",
    "                'test_title': df_test.iloc[idx]['title'],\n",
    "                'test_artist': df_test.iloc[idx]['artist_name'],\n",
    "                'test_genre': df_test.iloc[idx]['predicted_genre'],\n",
    "                'recommended_track_ids': ';'.join(match_ids),\n",
    "                'recommended_titles': ';'.join(match_titles),\n",
    "                'recommended_artists': ';'.join(match_artists),\n",
    "                'recommended_genres': ';'.join(match_genres),\n",
    "                'hit': hit\n",
    "            }])\n",
    "            line.to_csv(f, index=False, header=header)\n",
    "            f.flush()\n",
    "            os.fsync(f.fileno())\n",
    "            header = False\n",
    "\n",
    "    print(f\"Evaluation complete. Results saved to {output_file}\")\n",
    "\n",
    "# --- Run Inference and Save Output ---\n",
    "df_test = pd.read_csv(\"test_set.csv\")\n",
    "df_test, test_inputs = preprocess_test_df(df_test)\n",
    "test_embeddings = embedding_model.predict(test_inputs, batch_size=64, verbose=1)\n",
    "evaluate_recommendations(df_test, test_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Load CSVs\n",
    "df_rec = pd.read_csv(\"test_recommendations.csv\")\n",
    "df_test = pd.read_csv(\"test_set.csv\")\n",
    "df_test = df_test.set_index(\"track_id\")  # Easier access\n",
    "\n",
    "with open(\"formatted_recommendations.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for idx, row in df_rec.iterrows():\n",
    "        test_id = row['test_track_id']\n",
    "        f.write(f\"\\nüéµ Test Recommendation for: {row['test_title']} by {row['test_artist']} (Genre: {row['test_genre']})\\n\")\n",
    "        \n",
    "        # Ground truth top 5 similar songs\n",
    "        if test_id in df_test.index and isinstance(df_test.loc[test_id, \"top_5_similar_songs\"], str):\n",
    "            true_similars = set(df_test.loc[test_id, \"top_5_similar_songs\"].replace(\" \", \"\").split(\";\"))\n",
    "        else:\n",
    "            true_similars = set()\n",
    "\n",
    "        # Extract predicted recommendation data\n",
    "        rec_ids = row['recommended_track_ids'].split(';')\n",
    "        rec_titles = row['recommended_titles'].split(';')\n",
    "        rec_artists = row['recommended_artists'].split(';')\n",
    "        rec_genres = row['recommended_genres'].split(';')\n",
    "\n",
    "        # Combine and sort by overlap count\n",
    "        combined = list(zip(rec_ids, rec_titles, rec_artists, rec_genres))\n",
    "        combined.sort(key=lambda x: x[0] in true_similars, reverse=True)\n",
    "\n",
    "        # Build table\n",
    "        f.write(tabulate(combined, headers=[\"track_id\", \"title\", \"artist_name\", \"genre\"], tablefmt=\"github\"))\n",
    "        f.write(\"\\n\" + \"=\"*80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_hw_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
